{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a399a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-31 14:26:00.894124: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-31 14:26:00.894146: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import grid, imread\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.python.framework import ops\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c67716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_CleanUp():\n",
    "\n",
    "    grid_set = pd.read_csv(\"grid_set_small.txt\")\n",
    "    print('df read')\n",
    "    \n",
    "    m_set = pd.read_csv(\"m_set_small.txt\")\n",
    "    print('mandel read')\n",
    "\n",
    "    grid_set['i'] = grid_set['i'].astype(float)\n",
    "    grid_set['j'] = grid_set['j'].astype(float) \n",
    "    print('df to float is done')\n",
    "\n",
    "    m_set['i'] = m_set['i'].astype(float)\n",
    "    m_set['j'] = m_set['j'].astype(float) \n",
    "    print('mandel to float is done')\n",
    "    \n",
    "    grid_set['m_set_member'] = pd.Series([0 for x in range(len(grid_set.index))])\n",
    "    m_set['m_set_member'] = pd.Series([1 for x in range(len(m_set.index))])\n",
    "    print('binaries added')\n",
    "\n",
    "    grid_set.update(m_set)\n",
    "    m_set = 0\n",
    "    print('grid set update with m set binaries is done, and m_set set to 0 to preserve Mem')\n",
    "    \n",
    "    grid_set = grid_set.to_numpy()\n",
    "    print('to numpy done')\n",
    "    \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(grid_set[:,:2], grid_set[:,2], test_size=0.01, random_state=33)\n",
    "    print(\"split done\")\n",
    "    grid_set = 0\n",
    "    print('grid_set set to 0')\n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df459bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-z))\n",
    "    cache = z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff6a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,z)\n",
    "    \n",
    "    assert(A.shape == z.shape)\n",
    "    \n",
    "    cache = z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3b20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c91494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a0400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    # x_exp = ...\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    # x_sum = ...\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    # s = ...\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    s= np.divide(x_exp,x_sum)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb5c7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "383bef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        #(≈ 2 lines of code)\n",
    "        # parameters['W' + str(l)] = ...\n",
    "        # parameters['b' + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c17530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # Z = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda4e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # Z, linear_cache = ...\n",
    "        # A, activation_cache = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # Z, linear_cache = ...\n",
    "        # A, activation_cache = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4120101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        #(≈ 2 lines of code)\n",
    "        # A, cache = ...\n",
    "        # caches ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    #(≈ 2 lines of code)\n",
    "    # AL, cache = ...\n",
    "    # caches ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a1e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    # (≈ 1 lines of code)\n",
    "    # cost = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    cost = -1/m * (np.dot(Y,np.log(AL.T)) + np.dot(1 - Y,np.log(1 - AL).T))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1790ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # dW = ...\n",
    "    # db = ... sum by the rows of dZ with keepdims=True\n",
    "    # dA_prev = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dW = 1 / m * (np.dot(dZ,A_prev.T))\n",
    "    db = 1 / m * (np.sum(dZ,axis = 1,keepdims = True))\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "000c78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # dZ =  ...\n",
    "        # dA_prev, dW, db =  ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # dZ =  ...\n",
    "        # dA_prev, dW, db =  ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d7567f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    #(≈ 2 lines of code)\n",
    "    for l in range(L):\n",
    "        # parameters[\"W\" + str(l+1)] = ...\n",
    "        # parameters[\"b\" + str(l+1)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+ str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\"+ str(l+1)]\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "803f5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    #(1 line of code)\n",
    "    # dAL = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    #(approx. 5 lines)\n",
    "    # current_cache = ...\n",
    "    # dA_prev_temp, dW_temp, db_temp = ...\n",
    "    # grads[\"dA\" + str(L-1)] = ...\n",
    "    # grads[\"dW\" + str(L)] = ...\n",
    "    # grads[\"db\" + str(L)] = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        #(approx. 5 lines)\n",
    "        # current_cache = ...\n",
    "        # dA_prev_temp, dW_temp, db_temp = ...\n",
    "        # grads[\"dA\" + str(l)] = ...\n",
    "        # grads[\"dW\" + str(l + 1)] = ...\n",
    "        # grads[\"db\" + str(l + 1)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f88750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    #(≈ 2 lines of code)\n",
    "    # A2, cache = ...\n",
    "    # predictions = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    A2, cache = forward_propagation(X,parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0652b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, params=None, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    if(params == None):\n",
    "        parameters = initialize_parameters_deep(layers_dims)\n",
    "    else:\n",
    "        parameters = params\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        #(≈ 1 line of code)\n",
    "        # AL, caches = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute cost.\n",
    "        #(≈ 1 line of code)\n",
    "        # cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "        # Backward propagation.\n",
    "        #(≈ 1 line of code)\n",
    "        # grads = ...    \n",
    "        # YOUR CODE STARTS HERE\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    " \n",
    "        # Update parameters.\n",
    "        #(≈ 1 line of code)\n",
    "        # parameters = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 5 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28859133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df read\n",
      "mandel read\n",
      "df to float is done\n",
      "mandel to float is done\n",
      "binaries added\n",
      "grid set update with m set binaries is done, and m_set set to 0 to preserve Mem\n",
      "to numpy done\n",
      "split done\n",
      "grid_set set to 0\n",
      "(2, 3960000)\n"
     ]
    }
   ],
   "source": [
    "Data_list = Data_CleanUp()\n",
    "X_train = Data_list[0].reshape((2,len(Data_list[0])))\n",
    "X_test = Data_list[1].reshape((2,len(Data_list[1])))\n",
    "y_train = Data_list[2].reshape((1,len(Data_list[2])))\n",
    "y_test = Data_list[3].reshape((1,len(Data_list[3])))\n",
    "Data_list = 0\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b18bf3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [len(X_train), 25, 7, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb13b1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931472124669755\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = L_layer_model(X_train, y_train, layers_dims, num_iterations = 1, print_cost = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ba6a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931472124669755\n",
      "Cost after iteration 3: 0.6580033659987377\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = L_layer_model(X_train, y_train, layers_dims, learning_rate=0.075, num_iterations = 4, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a23d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('config.param', 'wb') as param_file:\n",
    "    pickle.dump(parameters, param_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79fe17ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.param', 'rb') as param_file:\n",
    "    parameters = pickle.load(param_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3956ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.647147205562019\n",
      "Cost after iteration 5: 0.598593385072431\n",
      "Cost after iteration 10: 0.5582940456649892\n",
      "Cost after iteration 15: 0.5247289092607619\n",
      "Cost after iteration 19: 0.5018798364583975\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = L_layer_model(X_train, y_train, layers_dims, learning_rate=0.075, num_iterations = 20, params = parameters, print_cost = True)\n",
    "with open('config.param', 'wb') as param_file:\n",
    "    pickle.dump(parameters, param_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.4966519425449834\n",
      "Cost after iteration 5: 0.49412498890231893\n",
      "Cost after iteration 10: 0.49164103775312423\n",
      "Cost after iteration 15: 0.48919923819030714\n",
      "Cost after iteration 20: 0.48679875742837475\n",
      "Cost after iteration 25: 0.4844387804374199\n",
      "Cost after iteration 30: 0.4821185095816702\n",
      "Cost after iteration 35: 0.47983716426276024\n",
      "Cost after iteration 40: 0.4775939805678703\n",
      "Cost after iteration 45: 0.4753882109228566\n",
      "Cost after iteration 50: 0.4732191237504807\n",
      "Cost after iteration 55: 0.4710860031338263\n",
      "Cost after iteration 60: 0.46898814848498444\n",
      "Cost after iteration 65: 0.46692487421906426\n",
      "Cost after iteration 70: 0.46489550943358243\n",
      "Cost after iteration 75: 0.46289939759326687\n",
      "Cost after iteration 80: 0.46093589622030123\n",
      "Cost after iteration 85: 0.4590043765900257\n",
      "Cost after iteration 90: 0.45710422343209994\n",
      "Cost after iteration 95: 0.4552348346371288\n",
      "Cost after iteration 100: 0.4533956209687335\n",
      "Cost after iteration 105: 0.45158600578105684\n",
      "Cost after iteration 110: 0.4498054247416744\n",
      "Cost after iteration 115: 0.4480533255598787\n",
      "Cost after iteration 120: 0.44632916772030473\n",
      "Cost after iteration 125: 0.44463242222184873\n",
      "Cost after iteration 130: 0.44296257132183664\n",
      "Cost after iteration 135: 0.44131910828539145\n",
      "Cost after iteration 140: 0.4397015371399422\n",
      "Cost after iteration 145: 0.4381093724348161\n",
      "Cost after iteration 150: 0.4365421390058574\n",
      "Cost after iteration 155: 0.4349993717450024\n",
      "Cost after iteration 160: 0.4334806153747462\n",
      "Cost after iteration 165: 0.43198542422743413\n",
      "Cost after iteration 170: 0.43051336202930574\n",
      "Cost after iteration 175: 0.4290640016892205\n",
      "Cost after iteration 180: 0.4276369250919893\n",
      "Cost after iteration 185: 0.42623172289624056\n",
      "Cost after iteration 190: 0.424847994336742\n",
      "Cost after iteration 195: 0.4234853470311074\n",
      "Cost after iteration 200: 0.42214339679080515\n",
      "Cost after iteration 205: 0.42082176743639693\n",
      "Cost after iteration 210: 0.4195200906169287\n",
      "Cost after iteration 215: 0.41823800563339164\n",
      "Cost after iteration 220: 0.4169751592661859\n",
      "Cost after iteration 225: 0.4157312056065007\n",
      "Cost after iteration 230: 0.4145058058915392\n",
      "Cost after iteration 235: 0.41329862834351216\n",
      "Cost after iteration 240: 0.4121093480123219\n",
      "Cost after iteration 245: 0.4109376466218654\n",
      "Cost after iteration 250: 0.40978321241987803\n",
      "Cost after iteration 255: 0.40864574003124754\n",
      "Cost after iteration 260: 0.40752493031472425\n",
      "Cost after iteration 265: 0.40642049022295484\n",
      "Cost after iteration 270: 0.4053321326657688\n",
      "Cost after iteration 275: 0.404259576376648\n",
      "Cost after iteration 280: 0.40320254578230896\n",
      "Cost after iteration 285: 0.4021607708753303\n",
      "Cost after iteration 290: 0.4011339870897574\n",
      "Cost after iteration 295: 0.4001219351796186\n",
      "Cost after iteration 300: 0.3991243611002872\n",
      "Cost after iteration 305: 0.3981410158926258\n",
      "Cost after iteration 310: 0.3971716555698471\n",
      "Cost after iteration 315: 0.39621604100703567\n",
      "Cost after iteration 320: 0.3952739378332624\n",
      "Cost after iteration 325: 0.39434511632623465\n",
      "Cost after iteration 330: 0.3934293513094268\n",
      "Cost after iteration 335: 0.3925264220516276\n",
      "Cost after iteration 340: 0.39163611216885247\n",
      "Cost after iteration 345: 0.39075820952856455\n",
      "Cost after iteration 350: 0.3898925061561473\n",
      "Cost after iteration 355: 0.38903879814358006\n",
      "Cost after iteration 360: 0.38819688556026255\n",
      "Cost after iteration 365: 0.3873665723659351\n",
      "Cost after iteration 370: 0.386547666325648\n",
      "Cost after iteration 375: 0.38573997892673173\n",
      "Cost after iteration 380: 0.38494332529771474\n",
      "Cost after iteration 385: 0.3841575241291488\n",
      "Cost after iteration 390: 0.38338239759629256\n",
      "Cost after iteration 395: 0.38261777128360863\n"
     ]
    }
   ],
   "source": [
    "with open('config.param', 'rb') as param_file:\n",
    "    parameters = pickle.load(param_file)\n",
    "\n",
    "parameters, costs = L_layer_model(X_train, y_train, layers_dims, num_iterations = 500, params = parameters, print_cost = True)\n",
    "\n",
    "with open('config.param', 'wb') as param_file:\n",
    "    pickle.dump(parameters, param_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(X_train, y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(X_test, y_test, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
